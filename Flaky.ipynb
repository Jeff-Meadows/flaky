{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Flaky\n",
    "\n",
    "**A plugin for nosetests or py.test that automatically reruns flaky tests.**\n",
    "\n",
    "`pip install flaky`\n",
    "\n",
    "Available on GitHub: https://github.com/box/flaky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why?\n",
    "\n",
    "Ever been tempted to do this?\n",
    "\n",
    "<img src=\"fixed_test.jpeg\" alt=\"'Fixed' Test\" width=\"400\" height=\"600\" />\n",
    "\n",
    "There are lots of *wrong* reasons to do something like this, but a test that fails only on occasion can really make you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Flaky Tests\n",
    "\n",
    "We call tests flaky when they:\n",
    "\n",
    "- Only fail sometimes\n",
    "- Have a failure mode that may be very difficult to fix\n",
    "- Often pass when rerun\n",
    "\n",
    "These tests put us in a bad position, and there's not a perfect fix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The mathematics of flaky tests\n",
    "\n",
    "Given 50 flaky tests (99% pass rate), how often will the test run fail?\n",
    "\n",
    "What about with 100 flaky tests?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The mathematics of flaky tests\n",
    "\n",
    "Given 50 flaky tests (99% pass rate), how often will the test run fail? **40%**\n",
    "\n",
    "\\begin{equation*}\n",
    "1 - \\left(1 - 0.01\\right)^{ 50} = 0.4\n",
    "\\end{equation*}\n",
    "\n",
    "What about with 100 flaky tests? **64%**\n",
    "\n",
    "\\begin{equation*}\n",
    "1 - \\left(1 - 0.01\\right)^{ 100} = 0.64\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The mathematics of rerun flaky tests\n",
    "\n",
    "If we retry those 50 flaky tests, however, the test run fails just **0.5%** of the time.\n",
    "\n",
    "\\begin{equation*}\n",
    "1 - \\left(1 - 0.01^2\\right)^{ 50} = 0.0005\n",
    "\\end{equation*}\n",
    "\n",
    "What about with 100 flaky tests? **1%**\n",
    "\n",
    "\\begin{equation*}\n",
    "1 - \\left(1 - 0.01^2\\right)^{ 100} = 0.001\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flaky_tests.py\n"
     ]
    }
   ],
   "source": [
    "%%file flaky_tests.py\n",
    "\n",
    "def test_list_length(my_list=[]):\n",
    "    my_list.append(0)\n",
    "    assert len(my_list) > 1 # Fails the first time it's run. Passes on subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\r\n",
      "======================================================================\r\n",
      "FAIL: flaky_tests.test_list_length\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/nose/case.py\", line 197, in runTest\r\n",
      "    self.test(*self.arg)\r\n",
      "  File \"/Users/jmeadows/Documents/Projects/pycon-flaky/flaky_tests.py\", line 4, in test_list_length\r\n",
      "    assert len(my_list) > 1 # Fails the first time it's run. Passes on subsequent runs.\r\n",
      "AssertionError\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.001s\r\n",
      "\r\n",
      "FAILED (failures=1)\r\n"
     ]
    }
   ],
   "source": [
    "!nosetests flaky_tests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If this test can't be fixed, you have a few options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This test would be pretty easy to fix, but many of us have encountered tests that rely on external components that might not have a solution that can be reached in the amount of time available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: fix this test next month when I have time\n",
    "#def test_list_length(my_list=[]):\n",
    "    #my_list.append(0)\n",
    "    #assert len(my_list) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### Comment out the flaky test\n",
    "\n",
    "Commenting out the flaky test makes sure it won't fail, but it leaves your code uncovered and defeats your tooling. No good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from unittest import skip\n",
    "\n",
    "@skip(\"Fix this text next month when I have time.\")\n",
    "def test_list_length(my_list=[]):\n",
    "    my_list.append(0)\n",
    "    assert len(my_list) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### Skip the flaky test\n",
    "\n",
    "Skipping the flaky test also makes sure it won't fail, but it's not really much better than commenting it out. These skipped tests are rarely fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What if we could rerun tests *automatically* when they fail?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Introducing **flaky**, a plugin for `nosetests` or `py.test` that automatically reruns flaky tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flaky_tests.py\n"
     ]
    }
   ],
   "source": [
    "%%file flaky_tests.py\n",
    "\n",
    "from flaky import flaky\n",
    "\n",
    "@flaky\n",
    "def test_list_length(my_list=[]):\n",
    "    my_list.append(0)\n",
    "    assert len(my_list) > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "===Flaky Test Report===\r\n",
      "\r\n",
      "test_list_length failed (1 runs remaining out of 2).\r\n",
      "\t<type 'exceptions.AssertionError'>\r\n",
      "\t\r\n",
      "\t<traceback object at 0x1052b4cb0>\r\n",
      "test_list_length passed 1 out of the required 1 times. Success!\r\n",
      "\r\n",
      "===End Flaky Test Report===\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.001s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!nosetests --with-flaky flaky_tests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Flaky\n",
    "\n",
    "**A plugin for nosetests or py.test that automatically reruns flaky tests.**\n",
    "\n",
    "Just mark your flaky tests with `@flaky`, and run them with `py.test` or `nosetests --with-flaky`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Customizing Flaky\n",
    "\n",
    "Need more reruns? No problem - just tell flaky how many times it can run the test:\n",
    "\n",
    "    @flaky(max_runs=5)\n",
    "    def test_list_length(my_list=[]):\n",
    "        my_list.append(0)\n",
    "        assert len(my_list) > 1\n",
    "\n",
    "Prefer to make sure the test can pass more than once? Tell flaky how many times it must pass:\n",
    "\n",
    "    @flaky(min_passes=2)\n",
    "    def test_list_length(my_list=[]):\n",
    "        my_list.append(0)\n",
    "        assert len(my_list) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Customizing Flaky\n",
    "\n",
    "Have a lot of flaky tests? Flaky allows you to mark an entire test class `@flaky`. Still not enough? Simply pass the command line argument `--force-flaky` to the test runner and all tests will be marked flaky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The flaky report\n",
    "\n",
    "The end of each test run includes some information about your flaky tests.\n",
    "\n",
    "This is important because a failed test won't be reported to the test runner if the rerun passes.\n",
    "\n",
    "However, if you don't want to see this info, you can pass `--no-flaky-report` to your test runner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flaky_nosetests.py\n"
     ]
    }
   ],
   "source": [
    "%%file flaky_nosetests.py\n",
    "\n",
    "from flaky import flaky\n",
    "from unittest import expectedFailure, TestCase\n",
    "\n",
    "class ExampleTests(TestCase):\n",
    "    _threshold = -1\n",
    "\n",
    "    def test_non_flaky_thing(self):\n",
    "        \"\"\"Flaky will not interact with this test\"\"\"\n",
    "        pass\n",
    "\n",
    "    @expectedFailure\n",
    "    def test_non_flaky_failing_thing(self):\n",
    "        \"\"\"Flaky will also not interact with this test\"\"\"\n",
    "        self.assertEqual(0, 1)\n",
    "\n",
    "    @flaky(3, 2)\n",
    "    def test_flaky_thing_that_fails_then_succeeds(self):\n",
    "        \"\"\"\n",
    "        Flaky will run this test 3 times.\n",
    "        It will fail once and then succeed twice.\n",
    "        \"\"\"\n",
    "        self._threshold += 1\n",
    "        if self._threshold < 1:\n",
    "            raise Exception(\"Threshold is not high enough: {0} vs {1}.\".format(\n",
    "                self._threshold, 1),\n",
    "            )\n",
    "\n",
    "    @flaky(3, 2)\n",
    "    def test_flaky_thing_that_succeeds_then_fails_then_succeeds(self):\n",
    "        \"\"\"\n",
    "        Flaky will run this test 3 times.\n",
    "        It will succeed once, fail once, and then succeed one more time.\n",
    "        \"\"\"\n",
    "        self._threshold += 1\n",
    "        if self._threshold == 1:\n",
    "            self.assertEqual(0, 1)\n",
    "\n",
    "    @flaky(2, 2)\n",
    "    def test_flaky_thing_that_always_passes(self):\n",
    "        \"\"\"Flaky will run this test twice.  Both will succeed.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/case.py:340: RuntimeWarning: TestResult has no addExpectedFailure method, reporting as passes\r\n",
      "  RuntimeWarning)\r\n",
      "..\r\n",
      "===Flaky Test Report===\r\n",
      "\r\n",
      "test_flaky_thing_that_always_passes passed 1 out of the required 2 times. Running test again until it passes 2 times.\r\n",
      "test_flaky_thing_that_always_passes passed 2 out of the required 2 times. Success!\r\n",
      "test_flaky_thing_that_fails_then_succeeds failed (2 runs remaining out of 3).\r\n",
      "\t<type 'exceptions.Exception'>\r\n",
      "\tThreshold is not high enough: 0 vs 1.\r\n",
      "\t<traceback object at 0x1052c43b0>\r\n",
      "test_flaky_thing_that_fails_then_succeeds passed 1 out of the required 2 times. Running test again until it passes 2 times.\r\n",
      "test_flaky_thing_that_fails_then_succeeds passed 2 out of the required 2 times. Success!\r\n",
      "test_flaky_thing_that_succeeds_then_fails_then_succeeds passed 1 out of the required 2 times. Running test again until it passes 2 times.\r\n",
      "test_flaky_thing_that_succeeds_then_fails_then_succeeds failed (1 runs remaining out of 3).\r\n",
      "\t<type 'exceptions.AssertionError'>\r\n",
      "\t0 != 1\r\n",
      "\t<traceback object at 0x1052c4638>\r\n",
      "test_flaky_thing_that_succeeds_then_fails_then_succeeds passed 2 out of the required 2 times. Success!\r\n",
      "\r\n",
      "===End Flaky Test Report===\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 5 tests in 0.002s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!nosetests --with-flaky flaky_nosetests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flaky_nosetests_2.py\n"
     ]
    }
   ],
   "source": [
    "%%file flaky_nosetests_2.py\n",
    "\n",
    "\n",
    "from flaky import flaky\n",
    "from unittest import TestCase\n",
    "\n",
    "@flaky\n",
    "class ExampleFlakyTests(TestCase):\n",
    "    _threshold = -1\n",
    "\n",
    "    def test_flaky_thing_that_fails_then_succeeds(self):\n",
    "        \"\"\"\n",
    "        Flaky will run this test twice.\n",
    "        It will fail once and then succeed.\n",
    "        \"\"\"\n",
    "        self._threshold += 1\n",
    "        if self._threshold < 1:\n",
    "            raise Exception(\"Threshold is not high enough: {0} vs {1}.\".format(\n",
    "                self._threshold, 1),\n",
    "            )\n",
    "\n",
    "\n",
    "@flaky\n",
    "def test_flaky_function(param=[]):\n",
    "    # pylint:disable=dangerous-default-value\n",
    "    param.append(None)\n",
    "    assert len(param) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "===Flaky Test Report===\r\n",
      "\r\n",
      "test_flaky_thing_that_fails_then_succeeds failed (1 runs remaining out of 2).\r\n",
      "\t<type 'exceptions.Exception'>\r\n",
      "\tThreshold is not high enough: 0 vs 1.\r\n",
      "\t<traceback object at 0x1049b7fc8>\r\n",
      "test_flaky_thing_that_fails_then_succeeds passed 1 out of the required 1 times. Success!\r\n",
      "test_flaky_function passed 1 out of the required 1 times. Success!\r\n",
      "\r\n",
      "===End Flaky Test Report===\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 2 tests in 0.001s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!nosetests --with-flaky flaky_nosetests_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flaky_pytest.py\n"
     ]
    }
   ],
   "source": [
    "%%file flaky_pytest.py\n",
    "\n",
    "from flaky import flaky\n",
    "from unittest import TestCase\n",
    "\n",
    "\n",
    "@flaky\n",
    "def test_something_flaky(dummy_list=[]):\n",
    "    # pylint:disable=dangerous-default-value\n",
    "    dummy_list.append(0)\n",
    "    assert len(dummy_list) > 1\n",
    "\n",
    "\n",
    "@flaky\n",
    "class TestExampleFlakyTests(object):\n",
    "    _threshold = -1\n",
    "\n",
    "    def test_flaky_thing_that_fails_then_succeeds(self):\n",
    "        \"\"\"\n",
    "        Flaky will run this test twice.\n",
    "        It will fail once and then succeed.\n",
    "        \"\"\"\n",
    "        self._threshold += 1\n",
    "        assert self._threshold >= 1\n",
    "\n",
    "\n",
    "@flaky\n",
    "class TestExampleFlakyTestCase(TestCase):\n",
    "    _threshold = -1\n",
    "\n",
    "    def test_flaky_thing_that_fails_then_succeeds(self):\n",
    "        \"\"\"\n",
    "        Flaky will run this test twice.\n",
    "        It will fail once and then succeed.\n",
    "        \"\"\"\n",
    "        self._threshold += 1\n",
    "        assert self._threshold >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 2.7.9 -- py-1.4.26 -- pytest-2.7.0\n",
      "rootdir: /Users/jmeadows/Documents/Projects/pycon-flaky, inifile: \n",
      "plugins: flaky\n",
      "collected 3 items \n",
      "\u001b[0m\n",
      "flaky_pytest.py ...\n",
      "===Flaky Test Report===\n",
      "\n",
      "test_something_flaky failed (1 runs remaining out of 2).\n",
      "\t<class '_pytest.assertion.reinterpret.AssertionError'>\n",
      "\tassert 1 > 1\n",
      " +  where 1 = len([0])\n",
      "\t[<TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/flaky/flaky_pytest_plugin.py:286>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/flaky/flaky_pytest_plugin.py:131>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:521>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:528>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:393>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:113>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:138>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:123>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:394>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/runner.py:90>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/python.py:1174>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:521>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:528>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:394>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/python.py:204>, <TracebackEntry /Users/jmeadows/Documents/Projects/pycon-flaky/flaky_pytest.py:10>]\n",
      "test_something_flaky passed 1 out of the required 1 times. Success!\n",
      "test_flaky_thing_that_fails_then_succeeds failed (1 runs remaining out of 2).\n",
      "\t<class '_pytest.assertion.reinterpret.AssertionError'>\n",
      "\tassert 0 >= 1\n",
      " +  where 0 = <flaky_pytest.TestExampleFlakyTests object at 0x104ba3dd0>._threshold\n",
      "\t[<TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/flaky/flaky_pytest_plugin.py:286>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/flaky/flaky_pytest_plugin.py:131>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:521>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:528>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:393>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:113>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:138>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:123>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:394>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/runner.py:90>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/python.py:1174>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:521>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:528>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:394>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/python.py:204>, <TracebackEntry /Users/jmeadows/Documents/Projects/pycon-flaky/flaky_pytest.py:23>]\n",
      "test_flaky_thing_that_fails_then_succeeds passed 1 out of the required 1 times. Success!\n",
      "test_flaky_thing_that_fails_then_succeeds failed (1 runs remaining out of 2).\n",
      "\t<class '_pytest.assertion.reinterpret.AssertionError'>\n",
      "\tassert 0 >= 1\n",
      " +  where 0 = <flaky_pytest.TestExampleFlakyTestCase testMethod=test_flaky_thing_that_fails_then_succeeds>._threshold\n",
      "\t[<TracebackEntry /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/case.py:329>, <TracebackEntry /Users/jmeadows/Documents/Projects/pycon-flaky/flaky_pytest.py:36>]\n",
      "test_flaky_thing_that_fails_then_succeeds passed 1 out of the required 1 times. Success!\n",
      "\n",
      "===End Flaky Test Report===\n",
      "\n",
      "\u001b[32m\u001b[1m=========================== 3 passed in 0.03 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test flaky_pytest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flaky_pytest_2.py\n"
     ]
    }
   ],
   "source": [
    "%%file flaky_pytest_2.py\n",
    "\n",
    "from flaky import flaky\n",
    "import pytest\n",
    "from unittest import TestCase\n",
    "\n",
    "class TestExample(object):\n",
    "    _threshold = -1\n",
    "\n",
    "    def test_non_flaky_thing(self):\n",
    "        \"\"\"Flaky will not interact with this test\"\"\"\n",
    "        pass\n",
    "\n",
    "    @pytest.mark.xfail\n",
    "    def test_non_flaky_failing_thing(self):\n",
    "        \"\"\"Flaky will also not interact with this test\"\"\"\n",
    "        assert self == 1\n",
    "\n",
    "    @flaky(3, 2)\n",
    "    def test_flaky_thing_that_fails_then_succeeds(self):\n",
    "        \"\"\"\n",
    "        Flaky will run this test 3 times.\n",
    "        It will fail once and then succeed twice.\n",
    "        \"\"\"\n",
    "        self._threshold += 1\n",
    "        assert self._threshold >= 1\n",
    "\n",
    "    @flaky(3, 2)\n",
    "    def test_flaky_thing_that_succeeds_then_fails_then_succeeds(self):\n",
    "        \"\"\"\n",
    "        Flaky will run this test 3 times.\n",
    "        It will succeed once, fail once, and then succeed one more time.\n",
    "        \"\"\"\n",
    "        self._threshold += 1\n",
    "        assert self._threshold != 1\n",
    "\n",
    "    @flaky(2, 2)\n",
    "    def test_flaky_thing_that_always_passes(self):\n",
    "        \"\"\"Flaky will run this test twice.  Both will succeed.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 2.7.9 -- py-1.4.26 -- pytest-2.7.0\n",
      "rootdir: /Users/jmeadows/Documents/Projects/pycon-flaky, inifile: \n",
      "plugins: flaky\n",
      "collected 5 items \n",
      "\u001b[0m\n",
      "flaky_pytest_2.py .x...\n",
      "===Flaky Test Report===\n",
      "\n",
      "test_flaky_thing_that_fails_then_succeeds failed (2 runs remaining out of 3).\n",
      "\t<class '_pytest.assertion.reinterpret.AssertionError'>\n",
      "\tassert 0 >= 1\n",
      " +  where 0 = <flaky_pytest_2.TestExample object at 0x10539f850>._threshold\n",
      "\t[<TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/flaky/flaky_pytest_plugin.py:286>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/flaky/flaky_pytest_plugin.py:131>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:521>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:528>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:393>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:113>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:138>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:123>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:394>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/runner.py:90>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/python.py:1174>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:521>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:528>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:394>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/python.py:204>, <TracebackEntry /Users/jmeadows/Documents/Projects/pycon-flaky/flaky_pytest_2.py:25>]\n",
      "test_flaky_thing_that_fails_then_succeeds passed 1 out of the required 2 times. Running test again until it passes 2 times.\n",
      "test_flaky_thing_that_fails_then_succeeds passed 2 out of the required 2 times. Success!\n",
      "test_flaky_thing_that_succeeds_then_fails_then_succeeds passed 1 out of the required 2 times. Running test again until it passes 2 times.\n",
      "test_flaky_thing_that_succeeds_then_fails_then_succeeds failed (1 runs remaining out of 3).\n",
      "\t<class '_pytest.assertion.reinterpret.AssertionError'>\n",
      "\tassert 1 != 1\n",
      " +  where 1 = <flaky_pytest_2.TestExample object at 0x10539fed0>._threshold\n",
      "\t[<TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/flaky/flaky_pytest_plugin.py:286>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/python.py:1174>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:521>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:528>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/core.py:394>, <TracebackEntry /Users/jmeadows/.virtualenvs/pycon-flaky/lib/python2.7/site-packages/_pytest/python.py:204>, <TracebackEntry /Users/jmeadows/Documents/Projects/pycon-flaky/flaky_pytest_2.py:34>]\n",
      "test_flaky_thing_that_succeeds_then_fails_then_succeeds passed 2 out of the required 2 times. Success!\n",
      "test_flaky_thing_that_always_passes passed 1 out of the required 2 times. Running test again until it passes 2 times.\n",
      "test_flaky_thing_that_always_passes passed 2 out of the required 2 times. Success!\n",
      "\n",
      "===End Flaky Test Report===\n",
      "\n",
      "\u001b[32m\u001b[1m===================== 4 passed, 1 xfailed in 0.03 seconds ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test flaky_pytest_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some advice\n",
    "\n",
    "Flaky, as great as it is, should be used with caution!\n",
    "\n",
    "Many failing tests are trying to tell you something! Don't just silence them with `@flaky`.\n",
    "\n",
    "When applied judiciously, flaky can be a great time-saving tool, but it can also be misused, so apply your best judgment."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
